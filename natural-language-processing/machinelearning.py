# deep-learning
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import confusion_matrix,classification_report
# text gen
import spacy
from keras.preprocessing.text import Tokenizer
import numpy as np
from keras.layers import LSTM, Embedding
from pickle import dump,load
import random
from random import randint
from keras.models import load_model
from keras.preprocessing.sequence import pad_sequences
# other imports
from tutor import Tutor
import pandas as pd
# sklearn modules
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.svm import LinearSVC  # https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html
from sklearn.pipeline import Pipeline
from exceptions import ImproperModelError
from sklearn.feature_extraction.text import TfidfVectorizer


# from the deep learning section of the
class DeepLearn:
    
    def deep_model(self, dataset):

        #iris = load_iris() reference this famous dataset if not universal attributes
        X = dataset.data
        y = to_categorical(dataset.target)
        #test and training data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
        #standardizing the data
        scaler_object = MinMaxScaler()
        scaler_object.fit(X_train)
        scaled_X_train = scaler_object.transform(X_train)
        scaled_X_test = scaler_object.transform(X_test)
        #building a simple neural network
        model = Sequential()
        model.add(Dense(8, input_dim=4, activation='relu'))
        model.add(Dense(8, input_dim=4, activation='relu'))
        model.add(Dense(3, activation='softmax'))
        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        print(model.summary())
        #fit the model
        model.fit(scaled_X_train,y_train, epochs=150, verbose=2) #epochs variable
        # Spits out probabilities by default
        # model.predict(scaled_X_test)
        model.predict_classes(scaled_X_test)
        #model.metrics_names for model metrics
        model.evaluate(x = scaled_X_test, y = y_test)
        predictions = model.predict_classes(scaled_X_test)
        y_test.argmax(axis=1)
        confusion_matrix(y_test.argmax(axis=1),predictions)
        classification_report(y_test.argmax(axis=1),predictions)
        model.save('{dataset}_dlmodel.h5')
        #newmodel = load_model('myfirstmodel.h5'), newmodel.predict_classes(X_test)
        
    
    #text_gen helper method 4
    def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):
        '''
        INPUTS:
        model : model that was trained on text data
        tokenizer : tokenizer that was fit on text data
        seq_len : length of training sequence
        seed_text : raw string text to serve as the seed
        num_gen_words : number of words to be generated by model
        '''
        
        # Final Output
        output_text = []
        # Intial Seed Sequence
        input_text = seed_text
        # Create num_gen_words
        for i in range(num_gen_words):
            # Take the input text string and encode it to a sequence
            encoded_text = tokenizer.texts_to_sequences([input_text])[0]
            # Pad sequences to our trained rate (50 words in the video)
            pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')
            # Predict Class Probabilities for each word
            pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]
            # Grab word
            pred_word = tokenizer.index_word[pred_word_ind] 
            # Update the sequence of input text (shifting one over with the new word)
            input_text += ' ' + pred_word
            output_text.append(pred_word)
        # Make it look like a sentence.
        return ' '.join(output_text)
        
    
    #language processor from spacy
    nlp = spacy.load('en',disable=['parser', 'tagger','ner'])

    def text_gen(file):
        
        d = read_file(file)
        tokens = separate_punc(d)
        # organize into sequences of tokens
        train_len = 25+1 # 50 training words , then one target word
        # Empty list of sequences
        text_sequences = []
        for i in range(train_len, len(tokens)):
            # Grab train_len# amount of characters
            # Add to list of sequences
            text_sequences.append(tokens[i-train_len:i])
        # integer encode sequences of words
        #this comes from keras.preprocessing.text import Tokenizer which has a tensor flow backend that causes problems on my machine 
        tokenizer = Tokenizer()
        tokenizer.fit_on_texts(text_sequences)
        sequences = tokenizer.texts_to_sequences(text_sequences)
        sequences = np.array(sequences)
        # First 49 words
        X = sequences[:,:-1]
        # last Word
        y = sequences[:,-1]
        y = to_categorical(y, num_classes=vocabulary_size+1)
        #training the model
        #define model
        model = create_model(vocabulary_size+1, seq_len)
        # fit model
        model.fit(X, y, batch_size=128, epochs=300,verbose=1)
        # save the model to file
        model.save('epochBIG.h5')
        # save the tokenizer
        dump(tokenizer, open('epochBIG', 'wb'))
        
        #generating new text
        random_pick = random.randint(0,len(text_sequences))
        random_seed_text = text_sequences[random_pick]
        seed_text = ' '.join(random_seed_text)
        generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)
        full_text = read_file('moby_dick_four_chapters.txt')
        for i,word in enumerate(full_text.split()):
             if word == 'inkling':
        print(' '.join(full_text.split()[i-20:i+20]))
        print('\n')


# from the text classification video
class ReadModule:

    def __init__(self):
        self.vocab_cache = {}
        self.vocab_index = 0

    # should return the data set, model used, predictions
    @staticmethod
    def train_test_model(model_name, file, correlation1, correlation2, prediction):
        # prep data
        ds = pd.read_csv(file, sep='\t')
        ReadingTutor.drop_blanks(ds)
        # train test split
        x_train, x_test, y_train, y_test = train_test_split(ds[[correlation1, correlation2]], ds[prediction],
                                                            test_size=0.33, random_state=42)
        if model_name is 'SVC':
            model = SVC(gamma='auto')
        elif model_name is 'LR':
            model = LogisticRegression(solver='lbfgs')
        elif model_name is 'NB':
            model = MultinomialNB()
        else:
            raise ImproperModelError('The Model Requested is not acceptable')
        model.fit(x_train, y_train)
        guesses = model.predict(x_test)
        conf_matrix = metrics.confusion_matrix(y_test, guesses)  # Print a confusion matrix
        class_report = metrics.classification_report(y_test, guesses)  # Print a classification report
        acc_score = metrics.accuracy_score(y_test, guesses)  # Print the overall accuracy
        return [conf_matrix, class_report, acc_score]

    # feature extraction
    def vectorize_words(self, file):
        # build a vocabulary
        vocab_vector = [file] + [0] * len(self.vocab_cache)
        with open(file, 'r') as f:
            f = f.read().lower().split()
        for word in f:
            if word in self.vocab_cache:
                vocab_vector[self.vocab_cache[word]] += 1
            else:
                self.vocab_cache[word] = self.vocab_index
                vocab_vector[self.vocab_cache[word]] += 1
                self.vocab_index += 1

    @staticmethod
    def count_vector(model_data):
        text_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC()), ])
        # Feed the training data through the pipeline
        text_clf.fit(model_data[1], model_data[3])
        # Form a prediction set
        return text_clf.predict(model_data[2])  # from here the metrics would be computed
